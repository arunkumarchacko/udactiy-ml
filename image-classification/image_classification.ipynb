{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "import numpy as np\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d8820f0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features, labels = helper.load_cfar10_batch(cifar10_dataset_folder_path, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 59  62  63]\n",
      "  [ 43  46  45]\n",
      "  [ 50  48  43]\n",
      "  ..., \n",
      "  [158 132 108]\n",
      "  [152 125 102]\n",
      "  [148 124 103]]\n",
      "\n",
      " [[ 16  20  20]\n",
      "  [  0   0   0]\n",
      "  [ 18   8   0]\n",
      "  ..., \n",
      "  [123  88  55]\n",
      "  [119  83  50]\n",
      "  [122  87  57]]\n",
      "\n",
      " [[ 25  24  21]\n",
      "  [ 16   7   0]\n",
      "  [ 49  27   8]\n",
      "  ..., \n",
      "  [118  84  50]\n",
      "  [120  84  50]\n",
      "  [109  73  42]]\n",
      "\n",
      " ..., \n",
      " [[208 170  96]\n",
      "  [201 153  34]\n",
      "  [198 161  26]\n",
      "  ..., \n",
      "  [160 133  70]\n",
      "  [ 56  31   7]\n",
      "  [ 53  34  20]]\n",
      "\n",
      " [[180 139  96]\n",
      "  [173 123  42]\n",
      "  [186 144  30]\n",
      "  ..., \n",
      "  [184 148  94]\n",
      "  [ 97  62  34]\n",
      "  [ 83  53  34]]\n",
      "\n",
      " [[177 144 116]\n",
      "  [168 129  94]\n",
      "  [179 142  87]\n",
      "  ..., \n",
      "  [216 184 140]\n",
      "  [151 118  84]\n",
      "  [123  92  72]]]\n",
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "print(features[0])\n",
    "print(labels[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return (x/255.0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    max_label = 9 #Assuming max is 9 since it is mentioned above.\n",
    "    # Not using LabelBinarizer since that will change based on the input.\n",
    "    for label in x:\n",
    "        if label > max_label:\n",
    "            print('Error: Unexpected label', label)\n",
    "        encoded = np.zeros(max_label + 1)\n",
    "        encoded[label] = 1\n",
    "        result.append(encoded)\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    print (image_shape)\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, image_shape[0], image_shape[1], image_shape[2]), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=[None,n_classes], name='y')\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 5, 10) (?, 8, 8, 10) (?, 4, 4, 10)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "#    print(conv_num_outputs)\n",
    "#    print(conv_ksize)\n",
    "#    print(conv_strides)\n",
    "#    print(pool_ksize)\n",
    "#    print(pool_strides)\n",
    "#    print(x_tensor.shape)\n",
    "#    print(x_tensor.shape[-1])\n",
    "    channels = tf.cast(x_tensor.shape[-1], tf.int32)\n",
    "#    channels = -1\n",
    "    W = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], channels, conv_num_outputs], stddev=0.1))\n",
    "    B = tf.Variable(tf.constant(0.1, tf.float32, [conv_num_outputs]))\n",
    "    \n",
    "    conv_node = tf.nn.conv2d(x_tensor, W, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME') + B\n",
    "    max_pool_node = tf.nn.max_pool(conv_node, [1,pool_ksize[0],pool_ksize[1],1], [1,pool_strides[0],pool_strides[1],1],padding='SAME')\n",
    "    print (W.shape, conv_node.shape, max_pool_node.shape)    \n",
    "    return max_pool_node\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10, 30, 6)\n",
      "(?, 1800) 10 30 6 [None, 10, 30, 6]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    print(x_tensor.shape)\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    row = shape[1]\n",
    "    col = shape[2]\n",
    "    layers = shape[3]\n",
    "    YY = tf.reshape(x_tensor, [-1, row*col*layers])\n",
    "    print(YY.shape, row, col, layers, shape)\n",
    "    return YY\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128)\n",
      "(128, 40) (40,) (?, 40)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    print(x_tensor.shape)\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "\n",
    "    W = tf.Variable(tf.truncated_normal([shape[1], num_outputs], stddev=0.1))\n",
    "    B = tf.Variable(tf.constant(0.1, tf.float32, [num_outputs]))\n",
    "    dense = tf.matmul(x_tensor, W) + B\n",
    "    \n",
    "    print(W.shape, B.shape, dense.shape)\n",
    "    return dense\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128)\n",
      "(128, 40) (40,) (?, 40)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return fully_conn(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "(3, 3, 3, 16) (?, 32, 32, 16) (?, 32, 32, 16)\n",
      "(3, 3, 16, 64) (?, 32, 32, 64) (?, 16, 16, 64)\n",
      "(3, 3, 64, 256) (?, 16, 16, 256) (?, 8, 8, 256)\n",
      "(3, 3, 256, 1024) (?, 8, 8, 1024) (?, 4, 4, 1024)\n",
      "(?, 4, 4, 1024)\n",
      "(?, 16384) 4 4 1024 [None, 4, 4, 1024]\n",
      "(?, 16384)\n",
      "(16384, 3076) (3076,) (?, 3076)\n",
      "(?, 3076)\n",
      "(3076, 1024) (1024,) (?, 1024)\n",
      "(?, 1024)\n",
      "(1024, 512) (512,) (?, 512)\n",
      "Conv1.shape (?, 32, 32, 16)\n",
      "Conv2.shape (?, 16, 16, 64)\n",
      "Conv3.shape (?, 8, 8, 256)\n",
      "Conv4.shape (?, 4, 4, 1024)\n",
      "fl3.shape (?, 512)\n",
      "(?, 512)\n",
      "(512, 10) (10,) (?, 10)\n",
      "(3, 3, 3, 16) (?, 32, 32, 16) (?, 32, 32, 16)\n",
      "(3, 3, 16, 64) (?, 32, 32, 64) (?, 16, 16, 64)\n",
      "(3, 3, 64, 256) (?, 16, 16, 256) (?, 8, 8, 256)\n",
      "(3, 3, 256, 1024) (?, 8, 8, 1024) (?, 4, 4, 1024)\n",
      "(?, 4, 4, 1024)\n",
      "(?, 16384) 4 4 1024 [None, 4, 4, 1024]\n",
      "(?, 16384)\n",
      "(16384, 3076) (3076,) (?, 3076)\n",
      "(?, 3076)\n",
      "(3076, 1024) (1024,) (?, 1024)\n",
      "(?, 1024)\n",
      "(1024, 512) (512,) (?, 512)\n",
      "Conv1.shape (?, 32, 32, 16)\n",
      "Conv2.shape (?, 16, 16, 64)\n",
      "Conv3.shape (?, 8, 8, 256)\n",
      "Conv4.shape (?, 4, 4, 1024)\n",
      "fl3.shape (?, 512)\n",
      "(?, 512)\n",
      "(512, 10) (10,) (?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    conv_num_outputs = 16\n",
    "    conv_ksize = (3,3)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (1,1)\n",
    "    pool_strides_2 = (2,2)\n",
    "\n",
    "    \n",
    "    conv1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2 = conv2d_maxpool(conv1, conv_num_outputs*4, conv_ksize, conv_strides, pool_ksize, pool_strides_2)\n",
    "    conv3 = conv2d_maxpool(conv2, conv_num_outputs*4*4, conv_ksize, conv_strides, pool_ksize, pool_strides_2)\n",
    "    conv4 = conv2d_maxpool(conv3, conv_num_outputs*4*4*4, conv_ksize, conv_strides, pool_ksize, pool_strides_2)\n",
    "    \n",
    "\n",
    "    flatten_layer = flatten(conv4)\n",
    "    \n",
    "    fl_do = tf.nn.dropout(flatten_layer, keep_prob)\n",
    "    fl1 = fully_conn(fl_do, 3076) \n",
    "    fl1_do = tf.nn.dropout(fl1, keep_prob)\n",
    "    fl2 = fully_conn(fl1_do, 1024) \n",
    "    fl2_do = tf.nn.dropout(fl2, keep_prob)\n",
    "    fl3 = fully_conn(fl2_do, 512) \n",
    "    fl3_do = tf.nn.dropout(fl3, keep_prob)\n",
    "    \n",
    "    print('Conv1.shape', conv1.shape)\n",
    "    print('Conv2.shape', conv2.shape)\n",
    "    print('Conv3.shape', conv3.shape)\n",
    "    print('Conv4.shape', conv4.shape)\n",
    "    print('fl3.shape', fl3.shape)\n",
    "\n",
    "    return output(fl3_do, len(valid_labels[0]))\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, {x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "\n",
    "    a, c = sess.run([accuracy, cost], {x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print(\": accuracy:\", str(a), \" loss: \", str(c))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  : accuracy: 0.4044  loss:  93.7558\n",
      "Epoch  2, CIFAR-10 Batch 1:  : accuracy: 0.4612  loss:  61.5784\n",
      "Epoch  3, CIFAR-10 Batch 1:  : accuracy: 0.4272  loss:  67.4167\n",
      "Epoch  4, CIFAR-10 Batch 1:  : accuracy: 0.5194  loss:  41.2361\n",
      "Epoch  5, CIFAR-10 Batch 1:  : accuracy: 0.5312  loss:  34.9791\n",
      "Epoch  6, CIFAR-10 Batch 1:  : accuracy: 0.5512  loss:  27.8366\n",
      "Epoch  7, CIFAR-10 Batch 1:  : accuracy: 0.5486  loss:  26.5119\n",
      "Epoch  8, CIFAR-10 Batch 1:  : accuracy: 0.5486  loss:  27.8253\n",
      "Epoch  9, CIFAR-10 Batch 1:  : accuracy: 0.5542  loss:  24.1274\n",
      "Epoch 10, CIFAR-10 Batch 1:  : accuracy: 0.562  loss:  21.7019\n",
      "Epoch 11, CIFAR-10 Batch 1:  : accuracy: 0.5664  loss:  20.5224\n",
      "Epoch 12, CIFAR-10 Batch 1:  : accuracy: 0.5714  loss:  20.0536\n",
      "Epoch 13, CIFAR-10 Batch 1:  : accuracy: 0.5536  loss:  22.1783\n",
      "Epoch 14, CIFAR-10 Batch 1:  : accuracy: 0.577  loss:  17.1621\n",
      "Epoch 15, CIFAR-10 Batch 1:  : accuracy: 0.5242  loss:  21.7935\n",
      "Epoch 16, CIFAR-10 Batch 1:  : accuracy: 0.5746  loss:  17.3038\n",
      "Epoch 17, CIFAR-10 Batch 1:  : accuracy: 0.592  loss:  14.8152\n",
      "Epoch 18, CIFAR-10 Batch 1:  : accuracy: 0.58  loss:  15.4958\n",
      "Epoch 19, CIFAR-10 Batch 1:  : accuracy: 0.5808  loss:  15.4155\n",
      "Epoch 20, CIFAR-10 Batch 1:  : accuracy: 0.6034  loss:  13.7709\n",
      "Epoch 21, CIFAR-10 Batch 1:  : accuracy: 0.5734  loss:  14.9566\n",
      "Epoch 22, CIFAR-10 Batch 1:  : accuracy: 0.5586  loss:  15.9318\n",
      "Epoch 23, CIFAR-10 Batch 1:  : accuracy: 0.604  loss:  13.0008\n",
      "Epoch 24, CIFAR-10 Batch 1:  : accuracy: 0.5682  loss:  14.2192\n",
      "Epoch 25, CIFAR-10 Batch 1:  : accuracy: 0.591  loss:  12.8146\n",
      "Epoch 26, CIFAR-10 Batch 1:  : accuracy: 0.584  loss:  12.875\n",
      "Epoch 27, CIFAR-10 Batch 1:  : accuracy: 0.599  loss:  12.4092\n",
      "Epoch 28, CIFAR-10 Batch 1:  : accuracy: 0.6006  loss:  11.653\n",
      "Epoch 29, CIFAR-10 Batch 1:  : accuracy: 0.5808  loss:  12.0847\n",
      "Epoch 30, CIFAR-10 Batch 1:  : accuracy: 0.6026  loss:  11.0471\n",
      "Epoch 31, CIFAR-10 Batch 1:  : accuracy: 0.5758  loss:  11.6989\n",
      "Epoch 32, CIFAR-10 Batch 1:  : accuracy: 0.61  loss:  10.3611\n",
      "Epoch 33, CIFAR-10 Batch 1:  : accuracy: 0.603  loss:  10.1089\n",
      "Epoch 34, CIFAR-10 Batch 1:  : accuracy: 0.592  loss:  10.1097\n",
      "Epoch 35, CIFAR-10 Batch 1:  : accuracy: 0.6032  loss:  10.045\n",
      "Epoch 36, CIFAR-10 Batch 1:  : accuracy: 0.5974  loss:  9.89952\n",
      "Epoch 37, CIFAR-10 Batch 1:  : accuracy: 0.6022  loss:  9.97224\n",
      "Epoch 38, CIFAR-10 Batch 1:  : accuracy: 0.5942  loss:  10.1918\n",
      "Epoch 39, CIFAR-10 Batch 1:  : accuracy: 0.5928  loss:  9.94858\n",
      "Epoch 40, CIFAR-10 Batch 1:  : accuracy: 0.6096  loss:  9.04884\n",
      "Epoch 41, CIFAR-10 Batch 1:  : accuracy: 0.5816  loss:  10.2446\n",
      "Epoch 42, CIFAR-10 Batch 1:  : accuracy: 0.6078  loss:  9.34369\n",
      "Epoch 43, CIFAR-10 Batch 1:  : accuracy: 0.6032  loss:  8.81919\n",
      "Epoch 44, CIFAR-10 Batch 1:  : accuracy: 0.5868  loss:  9.82222\n",
      "Epoch 45, CIFAR-10 Batch 1:  : accuracy: 0.587  loss:  10.7546\n",
      "Epoch 46, CIFAR-10 Batch 1:  : accuracy: 0.5756  loss:  9.6661\n",
      "Epoch 47, CIFAR-10 Batch 1:  : accuracy: 0.5846  loss:  9.99092\n",
      "Epoch 48, CIFAR-10 Batch 1:  : accuracy: 0.5794  loss:  9.7037\n",
      "Epoch 49, CIFAR-10 Batch 1:  : accuracy: 0.5914  loss:  9.73775\n",
      "Epoch 50, CIFAR-10 Batch 1:  : accuracy: 0.588  loss:  9.91524\n",
      "Epoch 51, CIFAR-10 Batch 1:  : accuracy: 0.5972  loss:  9.09684\n",
      "Epoch 52, CIFAR-10 Batch 1:  : accuracy: 0.5838  loss:  10.3211\n",
      "Epoch 53, CIFAR-10 Batch 1:  : accuracy: 0.5774  loss:  9.86314\n",
      "Epoch 54, CIFAR-10 Batch 1:  : accuracy: 0.591  loss:  9.51454\n",
      "Epoch 55, CIFAR-10 Batch 1:  : accuracy: 0.58  loss:  9.6925\n",
      "Epoch 56, CIFAR-10 Batch 1:  : accuracy: 0.577  loss:  11.1135\n",
      "Epoch 57, CIFAR-10 Batch 1:  : accuracy: 0.5728  loss:  10.4742\n",
      "Epoch 58, CIFAR-10 Batch 1:  : accuracy: 0.586  loss:  10.0784\n",
      "Epoch 59, CIFAR-10 Batch 1:  : accuracy: 0.5896  loss:  9.55218\n",
      "Epoch 60, CIFAR-10 Batch 1:  : accuracy: 0.5842  loss:  9.60353\n",
      "Epoch 61, CIFAR-10 Batch 1:  : accuracy: 0.58  loss:  10.0139\n",
      "Epoch 62, CIFAR-10 Batch 1:  : accuracy: 0.5854  loss:  9.62758\n",
      "Epoch 63, CIFAR-10 Batch 1:  : accuracy: 0.5776  loss:  10.9497\n",
      "Epoch 64, CIFAR-10 Batch 1:  : accuracy: 0.5988  loss:  8.75262\n",
      "Epoch 65, CIFAR-10 Batch 1:  : accuracy: 0.5718  loss:  11.1075\n",
      "Epoch 66, CIFAR-10 Batch 1:  : accuracy: 0.603  loss:  9.54489\n",
      "Epoch 67, CIFAR-10 Batch 1:  : accuracy: 0.6006  loss:  9.93866\n",
      "Epoch 68, CIFAR-10 Batch 1:  : accuracy: 0.5934  loss:  9.97315\n",
      "Epoch 69, CIFAR-10 Batch 1:  : accuracy: 0.576  loss:  11.5369\n",
      "Epoch 70, CIFAR-10 Batch 1:  : accuracy: 0.5794  loss:  11.3108\n",
      "Epoch 71, CIFAR-10 Batch 1:  : accuracy: 0.5844  loss:  10.9258\n",
      "Epoch 72, CIFAR-10 Batch 1:  : accuracy: 0.5912  loss:  11.3282\n",
      "Epoch 73, CIFAR-10 Batch 1:  : accuracy: 0.563  loss:  12.5645\n",
      "Epoch 74, CIFAR-10 Batch 1:  : accuracy: 0.5762  loss:  12.5171\n",
      "Epoch 75, CIFAR-10 Batch 1:  : accuracy: 0.5246  loss:  15.8136\n",
      "Epoch 76, CIFAR-10 Batch 1:  : accuracy: 0.5738  loss:  13.0455\n",
      "Epoch 77, CIFAR-10 Batch 1:  : accuracy: 0.5692  loss:  13.8867\n",
      "Epoch 78, CIFAR-10 Batch 1:  : accuracy: 0.5772  loss:  13.528\n",
      "Epoch 79, CIFAR-10 Batch 1:  : accuracy: 0.573  loss:  13.8299\n",
      "Epoch 80, CIFAR-10 Batch 1:  : accuracy: 0.5492  loss:  16.455\n",
      "Epoch 81, CIFAR-10 Batch 1:  : accuracy: 0.5584  loss:  16.1287\n",
      "Epoch 82, CIFAR-10 Batch 1:  : accuracy: 0.5398  loss:  19.3702\n",
      "Epoch 83, CIFAR-10 Batch 1:  : accuracy: 0.5816  loss:  15.0232\n",
      "Epoch 84, CIFAR-10 Batch 1:  : accuracy: 0.5636  loss:  16.5253\n",
      "Epoch 85, CIFAR-10 Batch 1:  : accuracy: 0.5784  loss:  17.1351\n",
      "Epoch 86, CIFAR-10 Batch 1:  : accuracy: 0.5126  loss:  21.3081\n",
      "Epoch 87, CIFAR-10 Batch 1:  : accuracy: 0.5596  loss:  22.2941\n",
      "Epoch 88, CIFAR-10 Batch 1:  : accuracy: 0.4566  loss:  32.7506\n",
      "Epoch 89, CIFAR-10 Batch 1:  : accuracy: 0.5598  loss:  21.1064\n",
      "Epoch 90, CIFAR-10 Batch 1:  : accuracy: 0.523  loss:  26.884\n",
      "Epoch 91, CIFAR-10 Batch 1:  : accuracy: 0.5412  loss:  26.8978\n",
      "Epoch 92, CIFAR-10 Batch 1:  : accuracy: 0.4926  loss:  32.8966\n",
      "Epoch 93, CIFAR-10 Batch 1:  : accuracy: 0.5464  loss:  27.4256\n",
      "Epoch 94, CIFAR-10 Batch 1:  : accuracy: 0.5544  loss:  29.3282\n",
      "Epoch 95, CIFAR-10 Batch 1:  : accuracy: 0.5466  loss:  28.3817\n",
      "Epoch 96, CIFAR-10 Batch 1:  : accuracy: 0.5704  loss:  28.7809\n",
      "Epoch 97, CIFAR-10 Batch 1:  : accuracy: 0.5728  loss:  28.2416\n",
      "Epoch 98, CIFAR-10 Batch 1:  : accuracy: 0.5428  loss:  34.7413\n",
      "Epoch 99, CIFAR-10 Batch 1:  : accuracy: 0.5388  loss:  37.3779\n",
      "Epoch 100, CIFAR-10 Batch 1:  : accuracy: 0.5426  loss:  35.6208\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  : accuracy: 0.4408  loss:  106.602\n",
      "Epoch  1, CIFAR-10 Batch 2:  : accuracy: 0.447  loss:  72.6894\n",
      "Epoch  1, CIFAR-10 Batch 3:  : accuracy: 0.4846  loss:  49.2405\n",
      "Epoch  1, CIFAR-10 Batch 4:  : accuracy: 0.5196  loss:  35.9262\n",
      "Epoch  1, CIFAR-10 Batch 5:  : accuracy: 0.5006  loss:  39.045\n",
      "Epoch  2, CIFAR-10 Batch 1:  : accuracy: 0.5584  loss:  25.3224\n",
      "Epoch  2, CIFAR-10 Batch 2:  : accuracy: 0.5546  loss:  24.0199\n",
      "Epoch  2, CIFAR-10 Batch 3:  : accuracy: 0.5716  loss:  20.2284\n",
      "Epoch  2, CIFAR-10 Batch 4:  : accuracy: 0.576  loss:  18.5927\n",
      "Epoch  2, CIFAR-10 Batch 5:  : accuracy: 0.5732  loss:  17.7902\n",
      "Epoch  3, CIFAR-10 Batch 1:  : accuracy: 0.5544  loss:  18.3367\n",
      "Epoch  3, CIFAR-10 Batch 2:  : accuracy: 0.5576  loss:  15.8219\n",
      "Epoch  3, CIFAR-10 Batch 3:  : accuracy: 0.5776  loss:  13.9028\n",
      "Epoch  3, CIFAR-10 Batch 4:  : accuracy: 0.5508  loss:  14.5454\n",
      "Epoch  3, CIFAR-10 Batch 5:  : accuracy: 0.5968  loss:  11.1611\n",
      "Epoch  4, CIFAR-10 Batch 1:  : accuracy: 0.5018  loss:  15.5454\n",
      "Epoch  4, CIFAR-10 Batch 2:  : accuracy: 0.5854  loss:  10.6153\n",
      "Epoch  4, CIFAR-10 Batch 3:  : accuracy: 0.5624  loss:  10.866\n",
      "Epoch  4, CIFAR-10 Batch 4:  : accuracy: 0.5992  loss:  8.07889\n",
      "Epoch  4, CIFAR-10 Batch 5:  : accuracy: 0.597  loss:  7.88719\n",
      "Epoch  5, CIFAR-10 Batch 1:  : accuracy: 0.6148  loss:  6.88505\n",
      "Epoch  5, CIFAR-10 Batch 2:  : accuracy: 0.5798  loss:  7.7354\n",
      "Epoch  5, CIFAR-10 Batch 3:  : accuracy: 0.5764  loss:  6.85489\n",
      "Epoch  5, CIFAR-10 Batch 4:  : accuracy: 0.5892  loss:  5.65358\n",
      "Epoch  5, CIFAR-10 Batch 5:  : accuracy: 0.5874  loss:  5.25527\n",
      "Epoch  6, CIFAR-10 Batch 1:  : accuracy: 0.5814  loss:  5.69247\n",
      "Epoch  6, CIFAR-10 Batch 2:  : accuracy: 0.5856  loss:  4.94709\n",
      "Epoch  6, CIFAR-10 Batch 3:  : accuracy: 0.5942  loss:  4.34849\n",
      "Epoch  6, CIFAR-10 Batch 4:  : accuracy: 0.592  loss:  4.01803\n",
      "Epoch  6, CIFAR-10 Batch 5:  : accuracy: 0.5986  loss:  3.57128\n",
      "Epoch  7, CIFAR-10 Batch 1:  : accuracy: 0.5836  loss:  3.69082\n",
      "Epoch  7, CIFAR-10 Batch 2:  : accuracy: 0.5784  loss:  3.41293\n",
      "Epoch  7, CIFAR-10 Batch 3:  : accuracy: 0.5894  loss:  3.2178\n",
      "Epoch  7, CIFAR-10 Batch 4:  : accuracy: 0.5988  loss:  2.80682\n",
      "Epoch  7, CIFAR-10 Batch 5:  : accuracy: 0.5966  loss:  2.70327\n",
      "Epoch  8, CIFAR-10 Batch 1:  : accuracy: 0.595  loss:  2.6004\n",
      "Epoch  8, CIFAR-10 Batch 2:  : accuracy: 0.5622  loss:  2.59447\n",
      "Epoch  8, CIFAR-10 Batch 3:  : accuracy: 0.5694  loss:  2.85908\n",
      "Epoch  8, CIFAR-10 Batch 4:  : accuracy: 0.5572  loss:  2.90305\n",
      "Epoch  8, CIFAR-10 Batch 5:  : accuracy: 0.5778  loss:  2.33227\n",
      "Epoch  9, CIFAR-10 Batch 1:  : accuracy: 0.607  loss:  1.99335\n",
      "Epoch  9, CIFAR-10 Batch 2:  : accuracy: 0.6  loss:  1.88282\n",
      "Epoch  9, CIFAR-10 Batch 3:  : accuracy: 0.5896  loss:  1.94163\n",
      "Epoch  9, CIFAR-10 Batch 4:  : accuracy: 0.5976  loss:  1.83588\n",
      "Epoch  9, CIFAR-10 Batch 5:  : accuracy: 0.5778  loss:  1.85433\n",
      "Epoch 10, CIFAR-10 Batch 1:  : accuracy: 0.5978  loss:  1.71416\n",
      "Epoch 10, CIFAR-10 Batch 2:  : accuracy: 0.5672  loss:  1.78736\n",
      "Epoch 10, CIFAR-10 Batch 3:  : accuracy: 0.5906  loss:  1.73491\n",
      "Epoch 10, CIFAR-10 Batch 4:  : accuracy: 0.5778  loss:  1.54123\n",
      "Epoch 10, CIFAR-10 Batch 5:  : accuracy: 0.598  loss:  1.62583\n",
      "Epoch 11, CIFAR-10 Batch 1:  : accuracy: 0.6036  loss:  1.47783\n",
      "Epoch 11, CIFAR-10 Batch 2:  : accuracy: 0.5904  loss:  1.4886\n",
      "Epoch 11, CIFAR-10 Batch 3:  : accuracy: 0.59  loss:  1.47262\n",
      "Epoch 11, CIFAR-10 Batch 4:  : accuracy: 0.6044  loss:  1.47777\n",
      "Epoch 11, CIFAR-10 Batch 5:  : accuracy: 0.5922  loss:  1.35298\n",
      "Epoch 12, CIFAR-10 Batch 1:  : accuracy: 0.6002  loss:  1.32737\n",
      "Epoch 12, CIFAR-10 Batch 2:  : accuracy: 0.614  loss:  1.29251\n",
      "Epoch 12, CIFAR-10 Batch 3:  : accuracy: 0.6166  loss:  1.23482\n",
      "Epoch 12, CIFAR-10 Batch 4:  : accuracy: 0.6232  loss:  1.20758\n",
      "Epoch 12, CIFAR-10 Batch 5:  : accuracy: 0.5986  loss:  1.24542\n",
      "Epoch 13, CIFAR-10 Batch 1:  : accuracy: 0.6186  loss:  1.18907\n",
      "Epoch 13, CIFAR-10 Batch 2:  : accuracy: 0.6054  loss:  1.1948\n",
      "Epoch 13, CIFAR-10 Batch 3:  : accuracy: 0.6188  loss:  1.16627\n",
      "Epoch 13, CIFAR-10 Batch 4:  : accuracy: 0.6238  loss:  1.14322\n",
      "Epoch 13, CIFAR-10 Batch 5:  : accuracy: 0.6238  loss:  1.13449\n",
      "Epoch 14, CIFAR-10 Batch 1:  : accuracy: 0.6164  loss:  1.13288\n",
      "Epoch 14, CIFAR-10 Batch 2:  : accuracy: 0.6088  loss:  1.15681\n",
      "Epoch 14, CIFAR-10 Batch 3:  : accuracy: 0.6302  loss:  1.0999\n",
      "Epoch 14, CIFAR-10 Batch 4:  : accuracy: 0.626  loss:  1.10943\n",
      "Epoch 14, CIFAR-10 Batch 5:  : accuracy: 0.621  loss:  1.12344\n",
      "Epoch 15, CIFAR-10 Batch 1:  : accuracy: 0.6306  loss:  1.09574\n",
      "Epoch 15, CIFAR-10 Batch 2:  : accuracy: 0.6328  loss:  1.08394\n",
      "Epoch 15, CIFAR-10 Batch 3:  : accuracy: 0.6044  loss:  1.17461\n",
      "Epoch 15, CIFAR-10 Batch 4:  : accuracy: 0.599  loss:  1.19598\n",
      "Epoch 15, CIFAR-10 Batch 5:  : accuracy: 0.6012  loss:  1.22431\n",
      "Epoch 16, CIFAR-10 Batch 1:  : accuracy: 0.6194  loss:  1.14649\n",
      "Epoch 16, CIFAR-10 Batch 2:  : accuracy: 0.6156  loss:  1.18262\n",
      "Epoch 16, CIFAR-10 Batch 3:  : accuracy: 0.6094  loss:  1.19213\n",
      "Epoch 16, CIFAR-10 Batch 4:  : accuracy: 0.6178  loss:  1.13884\n",
      "Epoch 16, CIFAR-10 Batch 5:  : accuracy: 0.6226  loss:  1.1469\n",
      "Epoch 17, CIFAR-10 Batch 1:  : accuracy: 0.6278  loss:  1.12264\n",
      "Epoch 17, CIFAR-10 Batch 2:  : accuracy: 0.5874  loss:  1.25476\n",
      "Epoch 17, CIFAR-10 Batch 3:  : accuracy: 0.6206  loss:  1.15295\n",
      "Epoch 17, CIFAR-10 Batch 4:  : accuracy: 0.6028  loss:  1.17082\n",
      "Epoch 17, CIFAR-10 Batch 5:  : accuracy: 0.6308  loss:  1.10744\n",
      "Epoch 18, CIFAR-10 Batch 1:  : accuracy: 0.6194  loss:  1.16621\n",
      "Epoch 18, CIFAR-10 Batch 2:  : accuracy: 0.6026  loss:  1.20504\n",
      "Epoch 18, CIFAR-10 Batch 3:  : accuracy: 0.6212  loss:  1.1472\n",
      "Epoch 18, CIFAR-10 Batch 4:  : accuracy: 0.5864  loss:  1.35247\n",
      "Epoch 18, CIFAR-10 Batch 5:  : accuracy: 0.6222  loss:  1.17587\n",
      "Epoch 19, CIFAR-10 Batch 1:  : accuracy: 0.6318  loss:  1.129\n",
      "Epoch 19, CIFAR-10 Batch 2:  : accuracy: 0.5822  loss:  1.27717\n",
      "Epoch 19, CIFAR-10 Batch 3:  : accuracy: 0.5876  loss:  1.34925\n",
      "Epoch 19, CIFAR-10 Batch 4:  : accuracy: 0.6136  loss:  1.22622\n",
      "Epoch 19, CIFAR-10 Batch 5:  : accuracy: 0.632  loss:  1.17417\n",
      "Epoch 20, CIFAR-10 Batch 1:  : accuracy: 0.6174  loss:  1.23419\n",
      "Epoch 20, CIFAR-10 Batch 2:  : accuracy: 0.5948  loss:  1.32791\n",
      "Epoch 20, CIFAR-10 Batch 3:  : accuracy: 0.5916  loss:  1.32059\n",
      "Epoch 20, CIFAR-10 Batch 4:  : accuracy: 0.6116  loss:  1.26171\n",
      "Epoch 20, CIFAR-10 Batch 5:  : accuracy: 0.6122  loss:  1.2199\n",
      "Epoch 21, CIFAR-10 Batch 1:  : accuracy: 0.6152  loss:  1.21549\n",
      "Epoch 21, CIFAR-10 Batch 2:  : accuracy: 0.605  loss:  1.25929\n",
      "Epoch 21, CIFAR-10 Batch 3:  : accuracy: 0.5854  loss:  1.3622\n",
      "Epoch 21, CIFAR-10 Batch 4:  : accuracy: 0.6118  loss:  1.22177\n",
      "Epoch 21, CIFAR-10 Batch 5:  : accuracy: 0.6222  loss:  1.20241\n",
      "Epoch 22, CIFAR-10 Batch 1:  : accuracy: 0.6146  loss:  1.22828\n",
      "Epoch 22, CIFAR-10 Batch 2:  : accuracy: 0.6008  loss:  1.31221\n",
      "Epoch 22, CIFAR-10 Batch 3:  : accuracy: 0.6304  loss:  1.18991\n",
      "Epoch 22, CIFAR-10 Batch 4:  : accuracy: 0.6198  loss:  1.21729\n",
      "Epoch 22, CIFAR-10 Batch 5:  : accuracy: 0.608  loss:  1.26789\n",
      "Epoch 23, CIFAR-10 Batch 1:  : accuracy: 0.6314  loss:  1.18626\n",
      "Epoch 23, CIFAR-10 Batch 2:  : accuracy: 0.6128  loss:  1.24013\n",
      "Epoch 23, CIFAR-10 Batch 3:  : accuracy: 0.591  loss:  1.35161\n",
      "Epoch 23, CIFAR-10 Batch 4:  : accuracy: 0.6172  loss:  1.26531\n",
      "Epoch 23, CIFAR-10 Batch 5:  : accuracy: 0.619  loss:  1.2694\n",
      "Epoch 24, CIFAR-10 Batch 1:  : accuracy: 0.5834  loss:  1.35207\n",
      "Epoch 24, CIFAR-10 Batch 2:  : accuracy: 0.5908  loss:  1.37906\n",
      "Epoch 24, CIFAR-10 Batch 3:  : accuracy: 0.6156  loss:  1.23141\n",
      "Epoch 24, CIFAR-10 Batch 4:  : accuracy: 0.611  loss:  1.25337\n",
      "Epoch 24, CIFAR-10 Batch 5:  : accuracy: 0.6194  loss:  1.24777\n",
      "Epoch 25, CIFAR-10 Batch 1:  : accuracy: 0.5636  loss:  1.53582\n",
      "Epoch 25, CIFAR-10 Batch 2:  : accuracy: 0.6092  loss:  1.25449\n",
      "Epoch 25, CIFAR-10 Batch 3:  : accuracy: 0.5904  loss:  1.39818\n",
      "Epoch 25, CIFAR-10 Batch 4:  : accuracy: 0.5448  loss:  1.62578\n",
      "Epoch 25, CIFAR-10 Batch 5:  : accuracy: 0.58  loss:  1.46208\n",
      "Epoch 26, CIFAR-10 Batch 1:  : accuracy: 0.6022  loss:  1.33996\n",
      "Epoch 26, CIFAR-10 Batch 2:  : accuracy: 0.592  loss:  1.42328\n",
      "Epoch 26, CIFAR-10 Batch 3:  : accuracy: 0.6134  loss:  1.30191\n",
      "Epoch 26, CIFAR-10 Batch 4:  : accuracy: 0.6158  loss:  1.25263\n",
      "Epoch 26, CIFAR-10 Batch 5:  : accuracy: 0.606  loss:  1.35526\n",
      "Epoch 27, CIFAR-10 Batch 1:  : accuracy: 0.5864  loss:  1.35528\n",
      "Epoch 27, CIFAR-10 Batch 2:  : accuracy: 0.5666  loss:  1.41096\n",
      "Epoch 27, CIFAR-10 Batch 3:  : accuracy: 0.5024  loss:  1.80155\n",
      "Epoch 27, CIFAR-10 Batch 4:  : accuracy: 0.5858  loss:  1.38218\n",
      "Epoch 27, CIFAR-10 Batch 5:  : accuracy: 0.6238  loss:  1.26227\n",
      "Epoch 28, CIFAR-10 Batch 1:  : accuracy: 0.608  loss:  1.27394\n",
      "Epoch 28, CIFAR-10 Batch 2:  : accuracy: 0.5992  loss:  1.35457\n",
      "Epoch 28, CIFAR-10 Batch 3:  : accuracy: 0.596  loss:  1.36138\n",
      "Epoch 28, CIFAR-10 Batch 4:  : accuracy: 0.5904  loss:  1.40369\n",
      "Epoch 28, CIFAR-10 Batch 5:  : accuracy: 0.5994  loss:  1.38791\n",
      "Epoch 29, CIFAR-10 Batch 1:  : accuracy: 0.6142  loss:  1.32918\n",
      "Epoch 29, CIFAR-10 Batch 2:  : accuracy: 0.576  loss:  1.62646\n",
      "Epoch 29, CIFAR-10 Batch 3:  : accuracy: 0.5366  loss:  1.74281\n",
      "Epoch 29, CIFAR-10 Batch 4:  : accuracy: 0.6118  loss:  1.28828\n",
      "Epoch 29, CIFAR-10 Batch 5:  : accuracy: 0.5934  loss:  1.50892\n",
      "Epoch 30, CIFAR-10 Batch 1:  : accuracy: 0.5804  loss:  1.47947\n",
      "Epoch 30, CIFAR-10 Batch 2:  : accuracy: 0.1068  loss:  3371.08\n",
      "Epoch 30, CIFAR-10 Batch 3:  : accuracy: 0.3012  loss:  19.4857\n",
      "Epoch 30, CIFAR-10 Batch 4:  : accuracy: 0.3908  loss:  9.36832\n",
      "Epoch 30, CIFAR-10 Batch 5:  : accuracy: 0.4222  loss:  7.09768\n",
      "Epoch 31, CIFAR-10 Batch 1:  : accuracy: 0.4734  loss:  5.47101\n",
      "Epoch 31, CIFAR-10 Batch 2:  : accuracy: 0.4742  loss:  4.78531\n",
      "Epoch 31, CIFAR-10 Batch 3:  : accuracy: 0.5016  loss:  4.4305\n",
      "Epoch 31, CIFAR-10 Batch 4:  : accuracy: 0.5138  loss:  3.7735\n",
      "Epoch 31, CIFAR-10 Batch 5:  : accuracy: 0.5432  loss:  3.48701\n",
      "Epoch 32, CIFAR-10 Batch 1:  : accuracy: 0.5494  loss:  3.2607\n",
      "Epoch 32, CIFAR-10 Batch 2:  : accuracy: 0.5338  loss:  3.37072\n",
      "Epoch 32, CIFAR-10 Batch 3:  : accuracy: 0.5514  loss:  3.08901\n",
      "Epoch 32, CIFAR-10 Batch 4:  : accuracy: 0.5698  loss:  2.75001\n",
      "Epoch 32, CIFAR-10 Batch 5:  : accuracy: 0.5814  loss:  2.64189\n",
      "Epoch 33, CIFAR-10 Batch 1:  : accuracy: 0.5856  loss:  2.55833\n",
      "Epoch 33, CIFAR-10 Batch 2:  : accuracy: 0.5768  loss:  2.43595\n",
      "Epoch 33, CIFAR-10 Batch 3:  : accuracy: 0.5742  loss:  2.43934\n",
      "Epoch 33, CIFAR-10 Batch 4:  : accuracy: 0.5802  loss:  2.50426\n",
      "Epoch 33, CIFAR-10 Batch 5:  : accuracy: 0.5988  loss:  2.27891\n",
      "Epoch 34, CIFAR-10 Batch 1:  : accuracy: 0.6026  loss:  2.24038\n",
      "Epoch 34, CIFAR-10 Batch 2:  : accuracy: 0.6092  loss:  2.13765\n",
      "Epoch 34, CIFAR-10 Batch 3:  : accuracy: 0.6082  loss:  2.10248\n",
      "Epoch 34, CIFAR-10 Batch 4:  : accuracy: 0.6212  loss:  2.02418\n",
      "Epoch 34, CIFAR-10 Batch 5:  : accuracy: 0.62  loss:  2.03592\n",
      "Epoch 35, CIFAR-10 Batch 1:  : accuracy: 0.6276  loss:  1.87532\n",
      "Epoch 35, CIFAR-10 Batch 2:  : accuracy: 0.6222  loss:  1.94119\n",
      "Epoch 35, CIFAR-10 Batch 3:  : accuracy: 0.6182  loss:  1.97745\n",
      "Epoch 35, CIFAR-10 Batch 4:  : accuracy: 0.6304  loss:  1.84067\n",
      "Epoch 35, CIFAR-10 Batch 5:  : accuracy: 0.6212  loss:  1.93686\n",
      "Epoch 36, CIFAR-10 Batch 1:  : accuracy: 0.6378  loss:  1.76252\n",
      "Epoch 36, CIFAR-10 Batch 2:  : accuracy: 0.6392  loss:  1.73491\n",
      "Epoch 36, CIFAR-10 Batch 3:  : accuracy: 0.6298  loss:  1.77432\n",
      "Epoch 36, CIFAR-10 Batch 4:  : accuracy: 0.6458  loss:  1.63232\n",
      "Epoch 36, CIFAR-10 Batch 5:  : accuracy: 0.6386  loss:  1.68856\n",
      "Epoch 37, CIFAR-10 Batch 1:  : accuracy: 0.6406  loss:  1.6786\n",
      "Epoch 37, CIFAR-10 Batch 2:  : accuracy: 0.6378  loss:  1.71783\n",
      "Epoch 37, CIFAR-10 Batch 3:  : accuracy: 0.631  loss:  1.77228\n",
      "Epoch 37, CIFAR-10 Batch 4:  : accuracy: 0.6386  loss:  1.61957\n",
      "Epoch 37, CIFAR-10 Batch 5:  : accuracy: 0.6438  loss:  1.62301\n",
      "Epoch 38, CIFAR-10 Batch 1:  : accuracy: 0.6434  loss:  1.56362\n",
      "Epoch 38, CIFAR-10 Batch 2:  : accuracy: 0.6502  loss:  1.54634\n",
      "Epoch 38, CIFAR-10 Batch 3:  : accuracy: 0.6432  loss:  1.56663\n",
      "Epoch 38, CIFAR-10 Batch 4:  : accuracy: 0.6552  loss:  1.50695\n",
      "Epoch 38, CIFAR-10 Batch 5:  : accuracy: 0.6568  loss:  1.51342\n",
      "Epoch 39, CIFAR-10 Batch 1:  : accuracy: 0.6558  loss:  1.48757\n",
      "Epoch 39, CIFAR-10 Batch 2:  : accuracy: 0.6316  loss:  1.65641\n",
      "Epoch 39, CIFAR-10 Batch 3:  : accuracy: 0.6442  loss:  1.55255\n",
      "Epoch 39, CIFAR-10 Batch 4:  : accuracy: 0.6504  loss:  1.494\n",
      "Epoch 39, CIFAR-10 Batch 5:  : accuracy: 0.6566  loss:  1.41224\n",
      "Epoch 40, CIFAR-10 Batch 1:  : accuracy: 0.6602  loss:  1.41805\n",
      "Epoch 40, CIFAR-10 Batch 2:  : accuracy: 0.6576  loss:  1.42842\n",
      "Epoch 40, CIFAR-10 Batch 3:  : accuracy: 0.6602  loss:  1.43303\n",
      "Epoch 40, CIFAR-10 Batch 4:  : accuracy: 0.6622  loss:  1.38333\n",
      "Epoch 40, CIFAR-10 Batch 5:  : accuracy: 0.6616  loss:  1.37947\n",
      "Epoch 41, CIFAR-10 Batch 1:  : accuracy: 0.6686  loss:  1.34495\n",
      "Epoch 41, CIFAR-10 Batch 2:  : accuracy: 0.6418  loss:  1.46496\n",
      "Epoch 41, CIFAR-10 Batch 3:  : accuracy: 0.6504  loss:  1.38568\n",
      "Epoch 41, CIFAR-10 Batch 4:  : accuracy: 0.6662  loss:  1.29892\n",
      "Epoch 41, CIFAR-10 Batch 5:  : accuracy: 0.6508  loss:  1.39512\n",
      "Epoch 42, CIFAR-10 Batch 1:  : accuracy: 0.6598  loss:  1.33322\n",
      "Epoch 42, CIFAR-10 Batch 2:  : accuracy: 0.6572  loss:  1.36004\n",
      "Epoch 42, CIFAR-10 Batch 3:  : accuracy: 0.6654  loss:  1.30394\n",
      "Epoch 42, CIFAR-10 Batch 4:  : accuracy: 0.6624  loss:  1.29112\n",
      "Epoch 42, CIFAR-10 Batch 5:  : accuracy: 0.6478  loss:  1.43973\n",
      "Epoch 43, CIFAR-10 Batch 1:  : accuracy: 0.6658  loss:  1.29438\n",
      "Epoch 43, CIFAR-10 Batch 2:  : accuracy: 0.6556  loss:  1.36003\n",
      "Epoch 43, CIFAR-10 Batch 3:  : accuracy: 0.662  loss:  1.27508\n",
      "Epoch 43, CIFAR-10 Batch 4:  : accuracy: 0.666  loss:  1.24909\n",
      "Epoch 43, CIFAR-10 Batch 5:  : accuracy: 0.6632  loss:  1.33885\n",
      "Epoch 44, CIFAR-10 Batch 1:  : accuracy: 0.6666  loss:  1.27043\n",
      "Epoch 44, CIFAR-10 Batch 2:  : accuracy: 0.639  loss:  1.42191\n",
      "Epoch 44, CIFAR-10 Batch 3:  : accuracy: 0.6778  loss:  1.22448\n",
      "Epoch 44, CIFAR-10 Batch 4:  : accuracy: 0.6674  loss:  1.27211\n",
      "Epoch 44, CIFAR-10 Batch 5:  : accuracy: 0.6678  loss:  1.25912\n",
      "Epoch 45, CIFAR-10 Batch 1:  : accuracy: 0.6682  loss:  1.26692\n",
      "Epoch 45, CIFAR-10 Batch 2:  : accuracy: 0.6588  loss:  1.25173\n",
      "Epoch 45, CIFAR-10 Batch 3:  : accuracy: 0.6746  loss:  1.21161\n",
      "Epoch 45, CIFAR-10 Batch 4:  : accuracy: 0.6796  loss:  1.19008\n",
      "Epoch 45, CIFAR-10 Batch 5:  : accuracy: 0.673  loss:  1.24623\n",
      "Epoch 46, CIFAR-10 Batch 1:  : accuracy: 0.668  loss:  1.22657\n",
      "Epoch 46, CIFAR-10 Batch 2:  : accuracy: 0.646  loss:  1.32849\n",
      "Epoch 46, CIFAR-10 Batch 3:  : accuracy: 0.6636  loss:  1.25067\n",
      "Epoch 46, CIFAR-10 Batch 4:  : accuracy: 0.6704  loss:  1.21739\n",
      "Epoch 46, CIFAR-10 Batch 5:  : accuracy: 0.6582  loss:  1.27322\n",
      "Epoch 47, CIFAR-10 Batch 1:  : accuracy: 0.6698  loss:  1.22095\n",
      "Epoch 47, CIFAR-10 Batch 2:  : accuracy: 0.6594  loss:  1.24273\n",
      "Epoch 47, CIFAR-10 Batch 3:  : accuracy: 0.6612  loss:  1.22903\n",
      "Epoch 47, CIFAR-10 Batch 4:  : accuracy: 0.6648  loss:  1.22125\n",
      "Epoch 47, CIFAR-10 Batch 5:  : accuracy: 0.6634  loss:  1.24387\n",
      "Epoch 48, CIFAR-10 Batch 1:  : accuracy: 0.6744  loss:  1.15901\n",
      "Epoch 48, CIFAR-10 Batch 2:  : accuracy: 0.6624  loss:  1.26661\n",
      "Epoch 48, CIFAR-10 Batch 3:  : accuracy: 0.6696  loss:  1.23312\n",
      "Epoch 48, CIFAR-10 Batch 4:  : accuracy: 0.6734  loss:  1.19619\n",
      "Epoch 48, CIFAR-10 Batch 5:  : accuracy: 0.659  loss:  1.2338\n",
      "Epoch 49, CIFAR-10 Batch 1:  : accuracy: 0.678  loss:  1.15076\n",
      "Epoch 49, CIFAR-10 Batch 2:  : accuracy: 0.655  loss:  1.23805\n",
      "Epoch 49, CIFAR-10 Batch 3:  : accuracy: 0.6686  loss:  1.23572\n",
      "Epoch 49, CIFAR-10 Batch 4:  : accuracy: 0.6704  loss:  1.17431\n",
      "Epoch 49, CIFAR-10 Batch 5:  : accuracy: 0.6622  loss:  1.23462\n",
      "Epoch 50, CIFAR-10 Batch 1:  : accuracy: 0.6732  loss:  1.18805\n",
      "Epoch 50, CIFAR-10 Batch 2:  : accuracy: 0.6708  loss:  1.17414\n",
      "Epoch 50, CIFAR-10 Batch 3:  : accuracy: 0.6696  loss:  1.17917\n",
      "Epoch 50, CIFAR-10 Batch 4:  : accuracy: 0.6802  loss:  1.15267\n",
      "Epoch 50, CIFAR-10 Batch 5:  : accuracy: 0.6574  loss:  1.25649\n",
      "Epoch 51, CIFAR-10 Batch 1:  : accuracy: 0.6726  loss:  1.17462\n",
      "Epoch 51, CIFAR-10 Batch 2:  : accuracy: 0.662  loss:  1.19521\n",
      "Epoch 51, CIFAR-10 Batch 3:  : accuracy: 0.6634  loss:  1.18758\n",
      "Epoch 51, CIFAR-10 Batch 4:  : accuracy: 0.6742  loss:  1.21567\n",
      "Epoch 51, CIFAR-10 Batch 5:  : accuracy: 0.67  loss:  1.18234\n",
      "Epoch 52, CIFAR-10 Batch 1:  : accuracy: 0.6808  loss:  1.17058\n",
      "Epoch 52, CIFAR-10 Batch 2:  : accuracy: 0.6626  loss:  1.19789\n",
      "Epoch 52, CIFAR-10 Batch 3:  : accuracy: 0.6728  loss:  1.1818\n",
      "Epoch 52, CIFAR-10 Batch 4:  : accuracy: 0.6792  loss:  1.18876\n",
      "Epoch 52, CIFAR-10 Batch 5:  : accuracy: 0.6696  loss:  1.23616\n",
      "Epoch 53, CIFAR-10 Batch 1:  : accuracy: 0.646  loss:  1.30005\n",
      "Epoch 53, CIFAR-10 Batch 2:  : accuracy: 0.6836  loss:  1.15675\n",
      "Epoch 53, CIFAR-10 Batch 3:  : accuracy: 0.68  loss:  1.1726\n",
      "Epoch 53, CIFAR-10 Batch 4:  : accuracy: 0.6662  loss:  1.24505\n",
      "Epoch 53, CIFAR-10 Batch 5:  : accuracy: 0.6628  loss:  1.20441\n",
      "Epoch 54, CIFAR-10 Batch 1:  : accuracy: 0.6844  loss:  1.16702\n",
      "Epoch 54, CIFAR-10 Batch 2:  : accuracy: 0.6634  loss:  1.29752\n",
      "Epoch 54, CIFAR-10 Batch 3:  : accuracy: 0.6764  loss:  1.22249\n",
      "Epoch 54, CIFAR-10 Batch 4:  : accuracy: 0.6754  loss:  1.19239\n",
      "Epoch 54, CIFAR-10 Batch 5:  : accuracy: 0.6578  loss:  1.31847\n",
      "Epoch 55, CIFAR-10 Batch 1:  : accuracy: 0.6656  loss:  1.2441\n",
      "Epoch 55, CIFAR-10 Batch 2:  : accuracy: 0.6748  loss:  1.21048\n",
      "Epoch 55, CIFAR-10 Batch 3:  : accuracy: 0.6784  loss:  1.22395\n",
      "Epoch 55, CIFAR-10 Batch 4:  : accuracy: 0.6568  loss:  1.2914\n",
      "Epoch 55, CIFAR-10 Batch 5:  : accuracy: 0.664  loss:  1.29337\n",
      "Epoch 56, CIFAR-10 Batch 1:  : accuracy: 0.6744  loss:  1.26548\n",
      "Epoch 56, CIFAR-10 Batch 2:  : accuracy: 0.667  loss:  1.29826\n",
      "Epoch 56, CIFAR-10 Batch 3:  : accuracy: 0.6764  loss:  1.25077\n",
      "Epoch 56, CIFAR-10 Batch 4:  : accuracy: 0.6646  loss:  1.27838\n",
      "Epoch 56, CIFAR-10 Batch 5:  : accuracy: 0.6444  loss:  1.42127\n",
      "Epoch 57, CIFAR-10 Batch 1:  : accuracy: 0.6596  loss:  1.3735\n",
      "Epoch 57, CIFAR-10 Batch 2:  : accuracy: 0.6796  loss:  1.25653\n",
      "Epoch 57, CIFAR-10 Batch 3:  : accuracy: 0.661  loss:  1.36741\n",
      "Epoch 57, CIFAR-10 Batch 4:  : accuracy: 0.6596  loss:  1.41284\n",
      "Epoch 57, CIFAR-10 Batch 5:  : accuracy: 0.6562  loss:  1.41075\n",
      "Epoch 58, CIFAR-10 Batch 1:  : accuracy: 0.6642  loss:  1.39866\n",
      "Epoch 58, CIFAR-10 Batch 2:  : accuracy: 0.6676  loss:  1.3356\n",
      "Epoch 58, CIFAR-10 Batch 3:  : accuracy: 0.6674  loss:  1.40947\n",
      "Epoch 58, CIFAR-10 Batch 4:  : accuracy: 0.6606  loss:  1.45527\n",
      "Epoch 58, CIFAR-10 Batch 5:  : accuracy: 0.6392  loss:  1.49642\n",
      "Epoch 59, CIFAR-10 Batch 1:  : accuracy: 0.6454  loss:  1.52095\n",
      "Epoch 59, CIFAR-10 Batch 2:  : accuracy: 0.6562  loss:  1.48092\n",
      "Epoch 59, CIFAR-10 Batch 3:  : accuracy: 0.6688  loss:  1.39934\n",
      "Epoch 59, CIFAR-10 Batch 4:  : accuracy: 0.6442  loss:  1.55885\n",
      "Epoch 59, CIFAR-10 Batch 5:  : accuracy: 0.6166  loss:  1.86405\n",
      "Epoch 60, CIFAR-10 Batch 1:  : accuracy: 0.6436  loss:  1.63053\n",
      "Epoch 60, CIFAR-10 Batch 2:  : accuracy: 0.629  loss:  1.73961\n",
      "Epoch 60, CIFAR-10 Batch 3:  : accuracy: 0.6478  loss:  1.619\n",
      "Epoch 60, CIFAR-10 Batch 4:  : accuracy: 0.6568  loss:  1.63209\n",
      "Epoch 60, CIFAR-10 Batch 5:  : accuracy: 0.6148  loss:  2.0059\n",
      "Epoch 61, CIFAR-10 Batch 1:  : accuracy: 0.6304  loss:  1.84963\n",
      "Epoch 61, CIFAR-10 Batch 2:  : accuracy: 0.6002  loss:  1.981\n",
      "Epoch 61, CIFAR-10 Batch 3:  : accuracy: 0.6336  loss:  1.79276\n",
      "Epoch 61, CIFAR-10 Batch 4:  : accuracy: 0.6024  loss:  2.05578\n",
      "Epoch 61, CIFAR-10 Batch 5:  : accuracy: 0.6338  loss:  1.95792\n",
      "Epoch 62, CIFAR-10 Batch 1:  : accuracy: 0.628  loss:  1.84815\n",
      "Epoch 62, CIFAR-10 Batch 2:  : accuracy: 0.6202  loss:  2.08185\n",
      "Epoch 62, CIFAR-10 Batch 3:  : accuracy: 0.6172  loss:  2.05196\n",
      "Epoch 62, CIFAR-10 Batch 4:  : accuracy: 0.621  loss:  1.90068\n",
      "Epoch 62, CIFAR-10 Batch 5:  : accuracy: 0.614  loss:  1.97578\n",
      "Epoch 63, CIFAR-10 Batch 1:  : accuracy: 0.6056  loss:  2.06124\n",
      "Epoch 63, CIFAR-10 Batch 2:  : accuracy: 0.6102  loss:  2.02736\n",
      "Epoch 63, CIFAR-10 Batch 3:  : accuracy: 0.635  loss:  1.95318\n",
      "Epoch 63, CIFAR-10 Batch 4:  : accuracy: 0.6114  loss:  2.25035\n",
      "Epoch 63, CIFAR-10 Batch 5:  : accuracy: 0.6434  loss:  2.04542\n",
      "Epoch 64, CIFAR-10 Batch 1:  : accuracy: 0.6054  loss:  2.32644\n",
      "Epoch 64, CIFAR-10 Batch 2:  : accuracy: 0.6284  loss:  2.19202\n",
      "Epoch 64, CIFAR-10 Batch 3:  : accuracy: 0.569  loss:  2.63458\n",
      "Epoch 64, CIFAR-10 Batch 4:  : accuracy: 0.597  loss:  2.45388\n",
      "Epoch 64, CIFAR-10 Batch 5:  : accuracy: 0.5832  loss:  2.97578\n",
      "Epoch 65, CIFAR-10 Batch 1:  : accuracy: 0.103  loss:  1733.48\n",
      "Epoch 65, CIFAR-10 Batch 2:  : accuracy: 0.2008  loss:  182.854\n",
      "Epoch 65, CIFAR-10 Batch 3:  : accuracy: 0.3262  loss:  31.2132\n",
      "Epoch 65, CIFAR-10 Batch 4:  : accuracy: 0.3954  loss:  18.4162\n",
      "Epoch 65, CIFAR-10 Batch 5:  : accuracy: 0.411  loss:  15.5603\n",
      "Epoch 66, CIFAR-10 Batch 1:  : accuracy: 0.472  loss:  10.8405\n",
      "Epoch 66, CIFAR-10 Batch 2:  : accuracy: 0.4732  loss:  10.245\n",
      "Epoch 66, CIFAR-10 Batch 3:  : accuracy: 0.4782  loss:  9.58727\n",
      "Epoch 66, CIFAR-10 Batch 4:  : accuracy: 0.5204  loss:  7.76808\n",
      "Epoch 66, CIFAR-10 Batch 5:  : accuracy: 0.5236  loss:  7.16551\n",
      "Epoch 67, CIFAR-10 Batch 1:  : accuracy: 0.5394  loss:  6.51126\n",
      "Epoch 67, CIFAR-10 Batch 2:  : accuracy: 0.5498  loss:  6.00156\n",
      "Epoch 67, CIFAR-10 Batch 3:  : accuracy: 0.5408  loss:  6.15641\n",
      "Epoch 67, CIFAR-10 Batch 4:  : accuracy: 0.5676  loss:  5.30659\n",
      "Epoch 67, CIFAR-10 Batch 5:  : accuracy: 0.5736  loss:  5.12439\n",
      "Epoch 68, CIFAR-10 Batch 1:  : accuracy: 0.5704  loss:  5.11309\n",
      "Epoch 68, CIFAR-10 Batch 2:  : accuracy: 0.5858  loss:  4.73038\n",
      "Epoch 68, CIFAR-10 Batch 3:  : accuracy: 0.5846  loss:  4.69366\n",
      "Epoch 68, CIFAR-10 Batch 4:  : accuracy: 0.5802  loss:  4.64443\n",
      "Epoch 68, CIFAR-10 Batch 5:  : accuracy: 0.5974  loss:  4.21209\n",
      "Epoch 69, CIFAR-10 Batch 1:  : accuracy: 0.598  loss:  4.2105\n",
      "Epoch 69, CIFAR-10 Batch 2:  : accuracy: 0.6028  loss:  4.1206\n",
      "Epoch 69, CIFAR-10 Batch 3:  : accuracy: 0.606  loss:  3.94939\n",
      "Epoch 69, CIFAR-10 Batch 4:  : accuracy: 0.6104  loss:  3.81593\n",
      "Epoch 69, CIFAR-10 Batch 5:  : accuracy: 0.607  loss:  3.99838\n",
      "Epoch 70, CIFAR-10 Batch 1:  : accuracy: 0.6136  loss:  3.87041\n",
      "Epoch 70, CIFAR-10 Batch 2:  : accuracy: 0.6128  loss:  3.68525\n",
      "Epoch 70, CIFAR-10 Batch 3:  : accuracy: 0.6266  loss:  3.49414\n",
      "Epoch 70, CIFAR-10 Batch 4:  : accuracy: 0.6204  loss:  3.55579\n",
      "Epoch 70, CIFAR-10 Batch 5:  : accuracy: 0.619  loss:  3.55825\n",
      "Epoch 71, CIFAR-10 Batch 1:  : accuracy: 0.6312  loss:  3.36562\n",
      "Epoch 71, CIFAR-10 Batch 2:  : accuracy: 0.6298  loss:  3.37281\n",
      "Epoch 71, CIFAR-10 Batch 3:  : accuracy: 0.6236  loss:  3.36009\n",
      "Epoch 71, CIFAR-10 Batch 4:  : accuracy: 0.632  loss:  3.2928\n",
      "Epoch 71, CIFAR-10 Batch 5:  : accuracy: 0.6296  loss:  3.30072\n",
      "Epoch 72, CIFAR-10 Batch 1:  : accuracy: 0.6374  loss:  3.16568\n",
      "Epoch 72, CIFAR-10 Batch 2:  : accuracy: 0.6368  loss:  3.1174\n",
      "Epoch 72, CIFAR-10 Batch 3:  : accuracy: 0.6404  loss:  3.07537\n",
      "Epoch 72, CIFAR-10 Batch 4:  : accuracy: 0.6404  loss:  3.05274\n",
      "Epoch 72, CIFAR-10 Batch 5:  : accuracy: 0.6446  loss:  2.93805\n",
      "Epoch 73, CIFAR-10 Batch 1:  : accuracy: 0.6382  loss:  3.10004\n",
      "Epoch 73, CIFAR-10 Batch 2:  : accuracy: 0.6336  loss:  3.00472\n",
      "Epoch 73, CIFAR-10 Batch 3:  : accuracy: 0.649  loss:  2.87482\n",
      "Epoch 73, CIFAR-10 Batch 4:  : accuracy: 0.6502  loss:  2.85684\n",
      "Epoch 73, CIFAR-10 Batch 5:  : accuracy: 0.6522  loss:  2.76684\n",
      "Epoch 74, CIFAR-10 Batch 1:  : accuracy: 0.6438  loss:  2.90032\n",
      "Epoch 74, CIFAR-10 Batch 2:  : accuracy: 0.6408  loss:  2.86217\n",
      "Epoch 74, CIFAR-10 Batch 3:  : accuracy: 0.649  loss:  2.76628\n",
      "Epoch 74, CIFAR-10 Batch 4:  : accuracy: 0.6476  loss:  2.76801\n",
      "Epoch 74, CIFAR-10 Batch 5:  : accuracy: 0.651  loss:  2.68433\n",
      "Epoch 75, CIFAR-10 Batch 1:  : accuracy: 0.645  loss:  2.81712\n",
      "Epoch 75, CIFAR-10 Batch 2:  : accuracy: 0.6526  loss:  2.68436\n",
      "Epoch 75, CIFAR-10 Batch 3:  : accuracy: 0.6536  loss:  2.63834\n",
      "Epoch 75, CIFAR-10 Batch 4:  : accuracy: 0.6558  loss:  2.63856\n",
      "Epoch 75, CIFAR-10 Batch 5:  : accuracy: 0.6512  loss:  2.5808\n",
      "Epoch 76, CIFAR-10 Batch 1:  : accuracy: 0.6506  loss:  2.56383\n",
      "Epoch 76, CIFAR-10 Batch 2:  : accuracy: 0.654  loss:  2.54164\n",
      "Epoch 76, CIFAR-10 Batch 3:  : accuracy: 0.654  loss:  2.51393\n",
      "Epoch 76, CIFAR-10 Batch 4:  : accuracy: 0.6594  loss:  2.54256\n",
      "Epoch 76, CIFAR-10 Batch 5:  : accuracy: 0.6582  loss:  2.45477\n",
      "Epoch 77, CIFAR-10 Batch 1:  : accuracy: 0.6522  loss:  2.55104\n",
      "Epoch 77, CIFAR-10 Batch 2:  : accuracy: 0.66  loss:  2.42337\n",
      "Epoch 77, CIFAR-10 Batch 3:  : accuracy: 0.656  loss:  2.41407\n",
      "Epoch 77, CIFAR-10 Batch 4:  : accuracy: 0.6514  loss:  2.52236\n",
      "Epoch 77, CIFAR-10 Batch 5:  : accuracy: 0.6582  loss:  2.37281\n",
      "Epoch 78, CIFAR-10 Batch 1:  : accuracy: 0.651  loss:  2.42616\n",
      "Epoch 78, CIFAR-10 Batch 2:  : accuracy: 0.6558  loss:  2.37917\n",
      "Epoch 78, CIFAR-10 Batch 3:  : accuracy: 0.6552  loss:  2.32643\n",
      "Epoch 78, CIFAR-10 Batch 4:  : accuracy: 0.6594  loss:  2.27257\n",
      "Epoch 78, CIFAR-10 Batch 5:  : accuracy: 0.6602  loss:  2.28108\n",
      "Epoch 79, CIFAR-10 Batch 1:  : accuracy: 0.6676  loss:  2.24156\n",
      "Epoch 79, CIFAR-10 Batch 2:  : accuracy: 0.6628  loss:  2.2244\n",
      "Epoch 79, CIFAR-10 Batch 3:  : accuracy: 0.6622  loss:  2.18817\n",
      "Epoch 79, CIFAR-10 Batch 4:  : accuracy: 0.6564  loss:  2.28315\n",
      "Epoch 79, CIFAR-10 Batch 5:  : accuracy: 0.6612  loss:  2.21018\n",
      "Epoch 80, CIFAR-10 Batch 1:  : accuracy: 0.6562  loss:  2.28243\n",
      "Epoch 80, CIFAR-10 Batch 2:  : accuracy: 0.6528  loss:  2.23855\n",
      "Epoch 80, CIFAR-10 Batch 3:  : accuracy: 0.666  loss:  2.10362\n",
      "Epoch 80, CIFAR-10 Batch 4:  : accuracy: 0.663  loss:  2.1289\n",
      "Epoch 80, CIFAR-10 Batch 5:  : accuracy: 0.6646  loss:  2.10029\n",
      "Epoch 81, CIFAR-10 Batch 1:  : accuracy: 0.6626  loss:  2.13274\n",
      "Epoch 81, CIFAR-10 Batch 2:  : accuracy: 0.662  loss:  2.11026\n",
      "Epoch 81, CIFAR-10 Batch 3:  : accuracy: 0.6694  loss:  2.04335\n",
      "Epoch 81, CIFAR-10 Batch 4:  : accuracy: 0.6648  loss:  2.03118\n",
      "Epoch 81, CIFAR-10 Batch 5:  : accuracy: 0.666  loss:  2.03041\n",
      "Epoch 82, CIFAR-10 Batch 1:  : accuracy: 0.6582  loss:  2.13002\n",
      "Epoch 82, CIFAR-10 Batch 2:  : accuracy: 0.6606  loss:  2.07777\n",
      "Epoch 82, CIFAR-10 Batch 3:  : accuracy: 0.6596  loss:  2.08061\n",
      "Epoch 82, CIFAR-10 Batch 4:  : accuracy: 0.6552  loss:  2.07391\n",
      "Epoch 82, CIFAR-10 Batch 5:  : accuracy: 0.6648  loss:  1.98681\n",
      "Epoch 83, CIFAR-10 Batch 1:  : accuracy: 0.6656  loss:  1.95374\n",
      "Epoch 83, CIFAR-10 Batch 2:  : accuracy: 0.6588  loss:  1.98218\n",
      "Epoch 83, CIFAR-10 Batch 3:  : accuracy: 0.6544  loss:  2.04232\n",
      "Epoch 83, CIFAR-10 Batch 4:  : accuracy: 0.667  loss:  1.95045\n",
      "Epoch 83, CIFAR-10 Batch 5:  : accuracy: 0.6604  loss:  1.97063\n",
      "Epoch 84, CIFAR-10 Batch 1:  : accuracy: 0.6692  loss:  1.96052\n",
      "Epoch 84, CIFAR-10 Batch 2:  : accuracy: 0.6654  loss:  1.93096\n",
      "Epoch 84, CIFAR-10 Batch 3:  : accuracy: 0.6558  loss:  1.96191\n",
      "Epoch 84, CIFAR-10 Batch 4:  : accuracy: 0.6672  loss:  1.91562\n",
      "Epoch 84, CIFAR-10 Batch 5:  : accuracy: 0.6626  loss:  1.89865\n",
      "Epoch 85, CIFAR-10 Batch 1:  : accuracy: 0.6622  loss:  1.9092\n",
      "Epoch 85, CIFAR-10 Batch 2:  : accuracy: 0.6658  loss:  1.89557\n",
      "Epoch 85, CIFAR-10 Batch 3:  : accuracy: 0.6712  loss:  1.84579\n",
      "Epoch 85, CIFAR-10 Batch 4:  : accuracy: 0.6746  loss:  1.84219\n",
      "Epoch 85, CIFAR-10 Batch 5:  : accuracy: 0.648  loss:  2.0657\n",
      "Epoch 86, CIFAR-10 Batch 1:  : accuracy: 0.6702  loss:  1.81104\n",
      "Epoch 86, CIFAR-10 Batch 2:  : accuracy: 0.6598  loss:  1.90324\n",
      "Epoch 86, CIFAR-10 Batch 3:  : accuracy: 0.6686  loss:  1.8216\n",
      "Epoch 86, CIFAR-10 Batch 4:  : accuracy: 0.6678  loss:  1.81561\n",
      "Epoch 86, CIFAR-10 Batch 5:  : accuracy: 0.6628  loss:  1.82654\n",
      "Epoch 87, CIFAR-10 Batch 1:  : accuracy: 0.674  loss:  1.76231\n",
      "Epoch 87, CIFAR-10 Batch 2:  : accuracy: 0.663  loss:  1.86022\n",
      "Epoch 87, CIFAR-10 Batch 3:  : accuracy: 0.6776  loss:  1.74977\n",
      "Epoch 87, CIFAR-10 Batch 4:  : accuracy: 0.6694  loss:  1.75205\n",
      "Epoch 87, CIFAR-10 Batch 5:  : accuracy: 0.667  loss:  1.76051\n",
      "Epoch 88, CIFAR-10 Batch 1:  : accuracy: 0.6752  loss:  1.76671\n",
      "Epoch 88, CIFAR-10 Batch 2:  : accuracy: 0.6524  loss:  1.84996\n",
      "Epoch 88, CIFAR-10 Batch 3:  : accuracy: 0.66  loss:  1.82549\n",
      "Epoch 88, CIFAR-10 Batch 4:  : accuracy: 0.6662  loss:  1.77398\n",
      "Epoch 88, CIFAR-10 Batch 5:  : accuracy: 0.6678  loss:  1.74795\n",
      "Epoch 89, CIFAR-10 Batch 1:  : accuracy: 0.6724  loss:  1.79549\n",
      "Epoch 89, CIFAR-10 Batch 2:  : accuracy: 0.669  loss:  1.81209\n",
      "Epoch 89, CIFAR-10 Batch 3:  : accuracy: 0.6716  loss:  1.72985\n",
      "Epoch 89, CIFAR-10 Batch 4:  : accuracy: 0.674  loss:  1.75649\n",
      "Epoch 89, CIFAR-10 Batch 5:  : accuracy: 0.665  loss:  1.75584\n",
      "Epoch 90, CIFAR-10 Batch 1:  : accuracy: 0.6746  loss:  1.73772\n",
      "Epoch 90, CIFAR-10 Batch 2:  : accuracy: 0.6686  loss:  1.76149\n",
      "Epoch 90, CIFAR-10 Batch 3:  : accuracy: 0.6626  loss:  1.80332\n",
      "Epoch 90, CIFAR-10 Batch 4:  : accuracy: 0.6668  loss:  1.81002\n",
      "Epoch 90, CIFAR-10 Batch 5:  : accuracy: 0.6638  loss:  1.80928\n",
      "Epoch 91, CIFAR-10 Batch 1:  : accuracy: 0.676  loss:  1.7601\n",
      "Epoch 91, CIFAR-10 Batch 2:  : accuracy: 0.6664  loss:  1.80674\n",
      "Epoch 91, CIFAR-10 Batch 3:  : accuracy: 0.6734  loss:  1.73992\n",
      "Epoch 91, CIFAR-10 Batch 4:  : accuracy: 0.667  loss:  1.80731\n",
      "Epoch 91, CIFAR-10 Batch 5:  : accuracy: 0.6648  loss:  1.78895\n",
      "Epoch 92, CIFAR-10 Batch 1:  : accuracy: 0.6708  loss:  1.80569\n",
      "Epoch 92, CIFAR-10 Batch 2:  : accuracy: 0.6582  loss:  1.86892\n",
      "Epoch 92, CIFAR-10 Batch 3:  : accuracy: 0.6766  loss:  1.76992\n",
      "Epoch 92, CIFAR-10 Batch 4:  : accuracy: 0.6728  loss:  1.81562\n",
      "Epoch 92, CIFAR-10 Batch 5:  : accuracy: 0.664  loss:  1.88726\n",
      "Epoch 93, CIFAR-10 Batch 1:  : accuracy: 0.6676  loss:  1.8124\n",
      "Epoch 93, CIFAR-10 Batch 2:  : accuracy: 0.6578  loss:  1.91762\n",
      "Epoch 93, CIFAR-10 Batch 3:  : accuracy: 0.6718  loss:  1.7894\n",
      "Epoch 93, CIFAR-10 Batch 4:  : accuracy: 0.6518  loss:  2.00927\n",
      "Epoch 93, CIFAR-10 Batch 5:  : accuracy: 0.6644  loss:  1.91254\n",
      "Epoch 94, CIFAR-10 Batch 1:  : accuracy: 0.6696  loss:  1.8746\n",
      "Epoch 94, CIFAR-10 Batch 2:  : accuracy: 0.6636  loss:  1.85413\n",
      "Epoch 94, CIFAR-10 Batch 3:  : accuracy: 0.67  loss:  1.93267\n",
      "Epoch 94, CIFAR-10 Batch 4:  : accuracy: 0.6732  loss:  1.8418\n",
      "Epoch 94, CIFAR-10 Batch 5:  : accuracy: 0.6636  loss:  1.95282\n",
      "Epoch 95, CIFAR-10 Batch 1:  : accuracy: 0.684  loss:  1.81516\n",
      "Epoch 95, CIFAR-10 Batch 2:  : accuracy: 0.6718  loss:  1.8818\n",
      "Epoch 95, CIFAR-10 Batch 3:  : accuracy: 0.6742  loss:  1.91889\n",
      "Epoch 95, CIFAR-10 Batch 4:  : accuracy: 0.6758  loss:  1.90036\n",
      "Epoch 95, CIFAR-10 Batch 5:  : accuracy: 0.6686  loss:  1.99537\n",
      "Epoch 96, CIFAR-10 Batch 1:  : accuracy: 0.6456  loss:  2.225\n",
      "Epoch 96, CIFAR-10 Batch 2:  : accuracy: 0.6604  loss:  2.16994\n",
      "Epoch 96, CIFAR-10 Batch 3:  : accuracy: 0.6738  loss:  2.03959\n",
      "Epoch 96, CIFAR-10 Batch 4:  : accuracy: 0.6498  loss:  2.18437\n",
      "Epoch 96, CIFAR-10 Batch 5:  : accuracy: 0.6628  loss:  2.11952\n",
      "Epoch 97, CIFAR-10 Batch 1:  : accuracy: 0.6598  loss:  2.1102\n",
      "Epoch 97, CIFAR-10 Batch 2:  : accuracy: 0.6554  loss:  2.16924\n",
      "Epoch 97, CIFAR-10 Batch 3:  : accuracy: 0.6552  loss:  2.31454\n",
      "Epoch 97, CIFAR-10 Batch 4:  : accuracy: 0.6604  loss:  2.22301\n",
      "Epoch 97, CIFAR-10 Batch 5:  : accuracy: 0.652  loss:  2.24822\n",
      "Epoch 98, CIFAR-10 Batch 1:  : accuracy: 0.672  loss:  2.16181\n",
      "Epoch 98, CIFAR-10 Batch 2:  : accuracy: 0.6374  loss:  2.56225\n",
      "Epoch 98, CIFAR-10 Batch 3:  : accuracy: 0.6304  loss:  2.67079\n",
      "Epoch 98, CIFAR-10 Batch 4:  : accuracy: 0.6532  loss:  2.49063\n",
      "Epoch 98, CIFAR-10 Batch 5:  : accuracy: 0.6528  loss:  2.53471\n",
      "Epoch 99, CIFAR-10 Batch 1:  : accuracy: 0.6468  loss:  2.58316\n",
      "Epoch 99, CIFAR-10 Batch 2:  : accuracy: 0.6376  loss:  2.58912\n",
      "Epoch 99, CIFAR-10 Batch 3:  : accuracy: 0.632  loss:  2.98223\n",
      "Epoch 99, CIFAR-10 Batch 4:  : accuracy: 0.6498  loss:  2.6797\n",
      "Epoch 99, CIFAR-10 Batch 5:  : accuracy: 0.6374  loss:  2.9697\n",
      "Epoch 100, CIFAR-10 Batch 1:  : accuracy: 0.6464  loss:  2.80212\n",
      "Epoch 100, CIFAR-10 Batch 2:  : accuracy: 0.6266  loss:  2.82225\n",
      "Epoch 100, CIFAR-10 Batch 3:  : accuracy: 0.6376  loss:  2.75069\n",
      "Epoch 100, CIFAR-10 Batch 4:  : accuracy: 0.616  loss:  2.94121\n",
      "Epoch 100, CIFAR-10 Batch 5:  : accuracy: 0.6294  loss:  3.15081\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6357792721518988\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130a4be48>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
